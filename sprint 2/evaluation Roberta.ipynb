{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cbcd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing RobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os \n",
    "import re\n",
    "from preprocessing import preprocessing_text\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "#### 1. import the Roberta Model\n",
    "\n",
    "roberta = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "#### 2. Preprocessing text\n",
    "\n",
    "from preprocessing import preprocessing_text\n",
    "\n",
    "#### 3. Functions to extract sentiment scores \n",
    "\n",
    "def extract_sentiment_score(text: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    extract the sentiment score for a input text\n",
    "    ----------\n",
    "    param: text\n",
    "    return: return tuple (labels, polarity)\n",
    "        labels: can be ['Negative', 'Neutral', 'Positive']\n",
    "        polarity: can be between 0 and 1\n",
    "    \"\"\"\n",
    "    # preprocessing\n",
    "    text = preprocessing_text(text)\n",
    "    #extract score\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = softmax(output[0][0].detach().numpy())\n",
    "    ranking = np.argsort(scores)[-1]\n",
    "    l = labels[ranking]\n",
    "    a = np.array([-1, 0, 1])\n",
    "    polarity = sum([i*j for i, j in zip(scores, a)])\n",
    "    return (l, polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea4a3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../csv_files/fuck_u.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc69805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns[[0, -1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b42113d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['target', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f21e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['target'].map({0 : 'Negative', 2 : 'Neutral', 4 : 'Positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9b02e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for _ in range(10):\n",
    "    df_light = df.sample(n=100)\n",
    "    lst_label = []\n",
    "    lst_score = []\n",
    "    count = 0\n",
    "    for text in df_light['text']:\n",
    "        count += 1\n",
    "        if count == 100 or count == 500 or count == 1000 or count == 2000:\n",
    "            print(count)\n",
    "        tup = extract_sentiment_score(text)\n",
    "        lst_label.append(tup[0])\n",
    "        lst_score.append(tup[1])\n",
    "    df_light.loc[:, 'prediction'] = lst_label\n",
    "    df_light.loc[:, 'sentiment_score'] = lst_score\n",
    "    df_light = df_light[df_light['prediction'] == 'Positive']\n",
    "    lst.append(sum(df_light['label'] == df_light['prediction'])/len(df_light))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e66f22fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7542794927041152"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lst)/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c49d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_light['label'] == df_light['prediction'])/len(df_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc1f8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('../../csv_files/ok_ok.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3b828d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id', 'label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2f21717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e2add52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>I sat through this whole movie just for Harry ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  Negative  Theo Walcott is still shit\\u002c watch Rafa an...\n",
       "1  Negative  its not that I\\u2019m a GSP fan\\u002c i just h...\n",
       "2  Negative  Iranian general says Israel\\u2019s Iron Dome c...\n",
       "3   Neutral  Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
       "4   Neutral  I sat through this whole movie just for Harry ..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'negative' : 'Negative', 'positive' : 'Positive', 'neutral' : 'Neutral'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32935e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for _ in range(10):\n",
    "    df_light = df.sample(n=100)\n",
    "    lst_label = []\n",
    "    lst_score = []\n",
    "    count = 0\n",
    "    for text in df_light['text']:\n",
    "        count += 1\n",
    "        if count == 100 or count == 500 or count == 1000 or count == 2000:\n",
    "            print(count)\n",
    "        tup = extract_sentiment_score(text)\n",
    "        lst_label.append(tup[0])\n",
    "        lst_score.append(tup[1])\n",
    "    df_light.loc[:, 'prediction'] = lst_label\n",
    "    df_light.loc[:, 'sentiment_score'] = lst_score\n",
    "#     df_light = df_light[df_light['prediction'] == 'Positive']\n",
    "    lst.append(sum(df_light['label'] == df_light['prediction'])/len(df_light))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa0de9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_light['label'] == df_light['prediction'])/len(df_light)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
