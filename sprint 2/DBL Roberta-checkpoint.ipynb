{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7c881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.0.53)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (1.20.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.1.96)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (4.59.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (20.9)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.8.1rc1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: hydra-core in c:\\users\\20211090\\appdata\\roaming\\python\\python38\\site-packages (1.0.7)\n",
      "Requirement already satisfied: omegaconf in c:\\users\\20211090\\appdata\\roaming\\python\\python38\\site-packages (2.0.6)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from hydra-core) (4.8)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from hydra-core) (5.7.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from omegaconf) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from omegaconf) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources->hydra-core) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20211090\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\20211090/.cache\\torch\\hub\\pytorch_fairseq_main\n",
      "2022-06-05 17:16:51 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-06-05 17:16:52 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at C:\\Users\\20211090\\.cache\\torch\\pytorch_fairseq\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "2022-06-05 17:16:55 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
      "2022-06-05 17:17:01 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_large', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='C:\\\\Users\\\\20211090\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19237, distributed_rank=0, distributed_world_size=1024, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0004], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=8, max_sentences_valid=8, max_source_positions=512, max_target_positions=512, max_tokens=4400, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=4, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': 'C:\\\\Users\\\\20211090\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "# pip install \n",
    "!pip install transformers==3.0.2\n",
    "!pip install regex requests hydra-core omegaconf\n",
    "import torch\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329b99ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.1.96)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.8.1rc1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.0.53)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (4.59.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (1.20.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: hydra-core in c:\\users\\20211090\\appdata\\roaming\\python\\python38\\site-packages (1.0.7)\n",
      "Requirement already satisfied: omegaconf in c:\\users\\20211090\\appdata\\roaming\\python\\python38\\site-packages (2.0.6)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from hydra-core) (5.7.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from hydra-core) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from omegaconf) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from omegaconf) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources->hydra-core) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\20211090/.cache\\torch\\hub\\pytorch_fairseq_main\n",
      "2022-06-05 17:17:08 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at C:\\Users\\20211090\\.cache\\torch\\pytorch_fairseq\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "2022-06-05 17:17:11 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
      "2022-06-05 17:17:17 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_large', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='C:\\\\Users\\\\20211090\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19237, distributed_rank=0, distributed_world_size=1024, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0004], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=8, max_sentences_valid=8, max_source_positions=512, max_target_positions=512, max_tokens=4400, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=4, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': 'C:\\\\Users\\\\20211090\\\\.cache\\\\torch\\\\pytorch_fairseq\\\\83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers==3.0.2\n",
    "# !pip install regex requests hydra-core omegaconf\n",
    "# import torch\n",
    "# from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from scipy.special import softmax\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pip install \n",
    "!pip install transformers==3.0.2\n",
    "!pip install regex requests hydra-core omegaconf\n",
    "import torch\n",
    "\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "\n",
    "#roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "#roberta = torch.hub.load('pytorch/fairseq', 'roberta.large', force_reload=True)\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.9.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "\n",
    "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n",
    "#from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ba98d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Pelusitaaaa1 Hoi!  Enkel KLM/Air France-vluch...</td>\n",
       "      <td>56377143</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Pelusitaaaa1 We helpen je graag. Kan je ons e...</td>\n",
       "      <td>56377143</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@addblackman We understand that you would pref...</td>\n",
       "      <td>56377143</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@CarlosMosig Con el fin de evitar errores noso...</td>\n",
       "      <td>56377143</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@addblackman We do understand that some passen...</td>\n",
       "      <td>56377143</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>Ã‚Â¡Ya vamos para Calella para hacer la activi...</td>\n",
       "      <td>749515688696483844</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>Home of Royal Dutch Airlines @KLM</td>\n",
       "      <td>31592791</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>@kjwoerkom @luchtvaart @KLM @tudelft Hij vlieg...</td>\n",
       "      <td>264190654</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>RT @SpaethFlies: V-shaped Flying Wing concept ...</td>\n",
       "      <td>759460420352630788</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>@Leodeadviseur @kjwoerkom @luchtvaart @KLM @tu...</td>\n",
       "      <td>856182297858641921</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5022 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text             user_id  \\\n",
       "0     @Pelusitaaaa1 Hoi!  Enkel KLM/Air France-vluch...            56377143   \n",
       "1     @Pelusitaaaa1 We helpen je graag. Kan je ons e...            56377143   \n",
       "2     @addblackman We understand that you would pref...            56377143   \n",
       "3     @CarlosMosig Con el fin de evitar errores noso...            56377143   \n",
       "4     @addblackman We do understand that some passen...            56377143   \n",
       "...                                                 ...                 ...   \n",
       "5017  Ã‚Â¡Ya vamos para Calella para hacer la activi...  749515688696483844   \n",
       "5018                  Home of Royal Dutch Airlines @KLM            31592791   \n",
       "5019  @kjwoerkom @luchtvaart @KLM @tudelft Hij vlieg...           264190654   \n",
       "5020  RT @SpaethFlies: V-shaped Flying Wing concept ...  759460420352630788   \n",
       "5021  @Leodeadviseur @kjwoerkom @luchtvaart @KLM @tu...  856182297858641921   \n",
       "\n",
       "     lang  \n",
       "0      nl  \n",
       "1      nl  \n",
       "2      en  \n",
       "3      es  \n",
       "4      en  \n",
       "...   ...  \n",
       "5017   es  \n",
       "5018   en  \n",
       "5019   nl  \n",
       "5020   en  \n",
       "5021   nl  \n",
       "\n",
       "[5022 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to the database\n",
    "con = sqlite3.connect(r\"C:\\Users\\20211090\\OneDrive\\Desktop\\database.db\")\n",
    "\n",
    "query_klm = '''\n",
    "    SELECT text, user_id, lang\n",
    "    FROM KLM_tweets\n",
    "'''\n",
    "df_klm = pd.read_sql_query(query_klm, con)\n",
    "\n",
    "\n",
    "df_klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d20081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change user-id from str to int\n",
    "df_klm = df_klm.astype({\n",
    "    'text' : 'str',\n",
    "    'user_id' : 'int64'                 \n",
    "}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541c8d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_klm.user_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909a021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>RT @McWhirterAlex: .@KLM will fly Amsterdam-Ba...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>RT @Vinamralongani: @KLM to launch thrice week...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>@wolfmandanny @KLM Thanks Dan</td>\n",
       "      <td>2409245946</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>@KLM Aviation releases about 3% of human carbo...</td>\n",
       "      <td>1109522354206584833</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>#worstserviceever DO NOT FLY EVER AGAIN @AirFr...</td>\n",
       "      <td>56459096</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>@KLM Thanks, I already went through this list ...</td>\n",
       "      <td>538129774</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>@MadelineDarveau @KLM Hi Maddie. I sincerely a...</td>\n",
       "      <td>5920532</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>@KLM Thank you, but I've contacted you guys al...</td>\n",
       "      <td>1918871</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>Home of Royal Dutch Airlines @KLM</td>\n",
       "      <td>31592791</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>RT @SpaethFlies: V-shaped Flying Wing concept ...</td>\n",
       "      <td>759460420352630788</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              user_id  \\\n",
       "921   RT @McWhirterAlex: .@KLM will fly Amsterdam-Ba...           1328529979   \n",
       "922   RT @Vinamralongani: @KLM to launch thrice week...           1328529979   \n",
       "923                       @wolfmandanny @KLM Thanks Dan           2409245946   \n",
       "925   @KLM Aviation releases about 3% of human carbo...  1109522354206584833   \n",
       "926   #worstserviceever DO NOT FLY EVER AGAIN @AirFr...             56459096   \n",
       "...                                                 ...                  ...   \n",
       "5010  @KLM Thanks, I already went through this list ...            538129774   \n",
       "5011  @MadelineDarveau @KLM Hi Maddie. I sincerely a...              5920532   \n",
       "5013  @KLM Thank you, but I've contacted you guys al...              1918871   \n",
       "5018                  Home of Royal Dutch Airlines @KLM             31592791   \n",
       "5020  RT @SpaethFlies: V-shaped Flying Wing concept ...   759460420352630788   \n",
       "\n",
       "     lang  \n",
       "921    en  \n",
       "922    en  \n",
       "923    en  \n",
       "925    en  \n",
       "926    en  \n",
       "...   ...  \n",
       "5010   en  \n",
       "5011   en  \n",
       "5013   en  \n",
       "5018   en  \n",
       "5020   en  \n",
       "\n",
       "[1862 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop KLM reply tweets and non_English tweets => so that we analyze only customers tweets in English\n",
    "df = df_klm[(df_klm.user_id != 56377143) & (df_klm.lang == 'en')] \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d7c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20211090\\AppData\\Local\\Temp\\ipykernel_2836\\42896380.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(preproc_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>. fly Amsterdam-Bangalore now that Jet Airways...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>launch thrice weekly service Amsterdam using a...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Thanks Dan</td>\n",
       "      <td>2409245946</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Aviation releases about 3% human carbon emissi...</td>\n",
       "      <td>1109522354206584833</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>DO NOT FLY EVER AGAIN OR How is one person alo...</td>\n",
       "      <td>56459096</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>Thanks, I already went through this list „š. S...</td>\n",
       "      <td>538129774</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>Hi Maddie. I sincerely apologize any inconveni...</td>\n",
       "      <td>5920532</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>Thank you, but I've contacted you guys already...</td>\n",
       "      <td>1918871</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>Home Royal Dutch Airlines</td>\n",
       "      <td>31592791</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>V-shaped Flying Wing concept by uses two engin...</td>\n",
       "      <td>759460420352630788</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              user_id  \\\n",
       "921   . fly Amsterdam-Bangalore now that Jet Airways...           1328529979   \n",
       "922   launch thrice weekly service Amsterdam using a...           1328529979   \n",
       "923                                          Thanks Dan           2409245946   \n",
       "925   Aviation releases about 3% human carbon emissi...  1109522354206584833   \n",
       "926   DO NOT FLY EVER AGAIN OR How is one person alo...             56459096   \n",
       "...                                                 ...                  ...   \n",
       "5010  Thanks, I already went through this list „š. S...            538129774   \n",
       "5011  Hi Maddie. I sincerely apologize any inconveni...              5920532   \n",
       "5013  Thank you, but I've contacted you guys already...              1918871   \n",
       "5018                          Home Royal Dutch Airlines             31592791   \n",
       "5020  V-shaped Flying Wing concept by uses two engin...   759460420352630788   \n",
       "\n",
       "     lang  \n",
       "921    en  \n",
       "922    en  \n",
       "923    en  \n",
       "925    en  \n",
       "926    en  \n",
       "...   ...  \n",
       "5010   en  \n",
       "5011   en  \n",
       "5013   en  \n",
       "5018   en  \n",
       "5020   en  \n",
       "\n",
       "[1862 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning text\n",
    "\n",
    "# define a function to prepocess the text (for KLM). \n",
    "def preproc_text(text):\n",
    "    text = re.sub(r'@[^\\s]+','',text) #remove quote\n",
    "    text = re.sub(r'RT[\\s]+',' ',text) #remove retweet\n",
    "    text = re.sub(r'http\\S+',' ',text) #remove hyperlink\n",
    "    text = re.sub(r'www.\\S+','',text) # remove web link\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+','',text) # remove hashtag\n",
    "    text = re.sub(r'[()!?]','',text) #remove exlamation marks\n",
    "    remove_list= 'Ã'+'Â'+'±'+'ã'+'¼'+'â'+'»'+'§'+ '¢‚¬¦'+ 'Å¸€¡³°Å¸€¡' + '¢‚¬Å¾'\n",
    "    table = str.maketrans('', '', remove_list)\n",
    "    text = text.translate(table)\n",
    "    \n",
    "    text = text.split()\n",
    "    del_words = ['for','of','and','to','from','on','in','at','a','an','the','am','will','would','was','were','shall']\n",
    "    text = [word for word in text if not word in del_words] #remove del_words\n",
    "    text = ' '.join(word for word in text)\n",
    "    return text\n",
    "  \n",
    "df['text'] = df['text'].apply(preproc_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf83281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral 0.7134866\n",
      "Neutral 0.92199904\n",
      "Positive 0.64728093\n",
      "Negative 0.8205634\n",
      "Negative 0.85023767\n",
      "Negative 0.5436127\n",
      "Negative 0.85023767\n",
      "Negative 0.57624537\n",
      "Neutral 0.69942534\n",
      "Positive 0.9572737\n",
      "Negative 0.48842427\n",
      "Negative 0.89696354\n",
      "Negative 0.9641954\n",
      "Positive 0.76235837\n",
      "Neutral 0.73965514\n",
      "Neutral 0.5253271\n",
      "Negative 0.6746364\n",
      "Positive 0.9824359\n",
      "Neutral 0.81622326\n",
      "Neutral 0.7903356\n",
      "Positive 0.903735\n",
      "Neutral 0.75144994\n",
      "Negative 0.7215495\n",
      "Neutral 0.8563944\n",
      "Neutral 0.47848138\n",
      "Neutral 0.47848138\n",
      "Positive 0.9466837\n",
      "Neutral 0.7134866\n",
      "Neutral 0.80940187\n",
      "Negative 0.94023144\n",
      "Negative 0.72089815\n",
      "Positive 0.8434891\n",
      "Neutral 0.5870702\n",
      "Neutral 0.7790671\n",
      "Neutral 0.9264088\n",
      "Neutral 0.9264088\n",
      "Negative 0.5345815\n",
      "Negative 0.5345815\n",
      "Negative 0.5345815\n",
      "Negative 0.5345815\n",
      "Neutral 0.4512725\n",
      "Neutral 0.4255923\n",
      "Neutral 0.4875894\n",
      "Neutral 0.46680674\n",
      "Negative 0.84891295\n",
      "Neutral 0.4255923\n",
      "Positive 0.92978424\n",
      "Negative 0.95088595\n",
      "Neutral 0.6973798\n",
      "Neutral 0.5197504\n",
      "Neutral 0.8284315\n",
      "Neutral 0.75534296\n",
      "Neutral 0.820293\n",
      "Neutral 0.47448066\n",
      "Positive 0.6567853\n",
      "Neutral 0.5516915\n",
      "Neutral 0.7790671\n",
      "Neutral 0.56149\n",
      "Positive 0.9900175\n",
      "Neutral 0.47448066\n",
      "Neutral 0.74833196\n",
      "Negative 0.8624175\n",
      "Neutral 0.86332744\n",
      "Neutral 0.85339046\n",
      "Neutral 0.81718373\n",
      "Positive 0.9920471\n",
      "Neutral 0.79046255\n",
      "Neutral 0.94501966\n",
      "Negative 0.5345815\n",
      "Negative 0.8180851\n",
      "Positive 0.9240979\n",
      "Neutral 0.89697313\n",
      "Neutral 0.91301054\n",
      "Positive 0.8236689\n",
      "Neutral 0.91301054\n",
      "Neutral 0.81913334\n",
      "Neutral 0.83758277\n",
      "Neutral 0.9086807\n",
      "Neutral 0.84426373\n",
      "Positive 0.6325503\n",
      "Positive 0.91467\n",
      "Negative 0.8872133\n",
      "Neutral 0.6203606\n",
      "Negative 0.9198447\n",
      "Positive 0.8136437\n",
      "Neutral 0.8313335\n",
      "Negative 0.43103236\n",
      "Negative 0.82167906\n",
      "Neutral 0.7750824\n",
      "Negative 0.5212581\n",
      "Positive 0.88457626\n",
      "Neutral 0.82367665\n",
      "Positive 0.98946387\n",
      "Negative 0.71941\n",
      "Negative 0.7982234\n",
      "Neutral 0.4144506\n",
      "Negative 0.50104976\n",
      "Neutral 0.82231116\n",
      "Positive 0.9800996\n",
      "Neutral 0.8465736\n",
      "Neutral 0.88245964\n",
      "Neutral 0.94501966\n",
      "Positive 0.8822987\n",
      "Positive 0.8714092\n",
      "Neutral 0.5018305\n",
      "Neutral 0.52762866\n",
      "Negative 0.8152166\n",
      "Negative 0.68895763\n",
      "Neutral 0.85245514\n",
      "Neutral 0.73835635\n",
      "Neutral 0.52942437\n",
      "Positive 0.93970805\n",
      "Negative 0.55346644\n",
      "Positive 0.92259204\n",
      "Neutral 0.7110526\n",
      "Positive 0.7724126\n",
      "Positive 0.9709102\n",
      "Neutral 0.48236784\n",
      "Positive 0.7036844\n",
      "Neutral 0.7640891\n",
      "Positive 0.62963504\n",
      "Negative 0.9312569\n",
      "Positive 0.9856477\n",
      "Positive 0.9912027\n",
      "Positive 0.9844809\n",
      "Neutral 0.3667198\n",
      "Neutral 0.70538545\n",
      "Negative 0.51139575\n",
      "Negative 0.8849791\n",
      "Negative 0.5890671\n",
      "Negative 0.8849791\n",
      "Positive 0.84221697\n",
      "Positive 0.6825734\n",
      "Positive 0.8223907\n",
      "Neutral 0.82347715\n",
      "Neutral 0.8417987\n",
      "Positive 0.9731473\n",
      "Neutral 0.77253157\n",
      "Positive 0.9903669\n",
      "Positive 0.5496158\n",
      "Positive 0.5500538\n",
      "Positive 0.5500538\n",
      "Positive 0.9806143\n",
      "Neutral 0.811793\n",
      "Neutral 0.5103132\n",
      "Positive 0.6454119\n",
      "Neutral 0.84716344\n",
      "Neutral 0.83596027\n",
      "Neutral 0.9245446\n",
      "Neutral 0.89727473\n",
      "Neutral 0.79164433\n",
      "Neutral 0.769474\n",
      "Negative 0.59989756\n",
      "Neutral 0.7608018\n",
      "Positive 0.89967585\n",
      "Neutral 0.9245183\n",
      "Positive 0.9749829\n",
      "Neutral 0.76814705\n",
      "Positive 0.89967585\n",
      "Positive 0.85149163\n",
      "Neutral 0.73021066\n",
      "Neutral 0.63072747\n",
      "Positive 0.9689661\n",
      "Positive 0.89967585\n",
      "Neutral 0.7430164\n",
      "Neutral 0.81816494\n",
      "Neutral 0.65662605\n",
      "Neutral 0.6709835\n",
      "Positive 0.8737609\n",
      "Neutral 0.93380255\n",
      "Neutral 0.84651905\n",
      "Positive 0.92833155\n",
      "Neutral 0.9297314\n",
      "Neutral 0.53385514\n",
      "Neutral 0.9057693\n",
      "Neutral 0.91588116\n",
      "Positive 0.5382838\n",
      "Neutral 0.91588116\n",
      "Positive 0.7070874\n",
      "Positive 0.9026047\n",
      "Neutral 0.91588116\n",
      "Negative 0.9660695\n",
      "Positive 0.89967585\n",
      "Negative 0.94424284\n",
      "Positive 0.9672569\n",
      "Neutral 0.64098626\n",
      "Positive 0.72244996\n",
      "Negative 0.5663775\n",
      "Positive 0.6956999\n",
      "Neutral 0.7928875\n",
      "Neutral 0.8037554\n",
      "Neutral 0.7529999\n",
      "Neutral 0.579061\n",
      "Negative 0.6331796\n",
      "Negative 0.7690854\n",
      "Negative 0.8268627\n",
      "Negative 0.75382227\n",
      "Negative 0.9647072\n",
      "Neutral 0.8543405\n",
      "Neutral 0.78763634\n",
      "Positive 0.9317467\n",
      "Neutral 0.55711716\n",
      "Negative 0.96090794\n",
      "Positive 0.91467345\n",
      "Negative 0.6250757\n",
      "Neutral 0.82251716\n",
      "Neutral 0.46866187\n",
      "Negative 0.9621467\n",
      "Negative 0.56417805\n",
      "Neutral 0.8810624\n",
      "Negative 0.5431271\n",
      "Neutral 0.7499664\n",
      "Positive 0.9364945\n",
      "Neutral 0.8029964\n",
      "Positive 0.44742194\n",
      "Neutral 0.5896961\n",
      "Positive 0.936952\n",
      "Positive 0.9133544\n",
      "Negative 0.5934607\n",
      "Neutral 0.9328797\n",
      "Positive 0.87646425\n",
      "Negative 0.6554847\n",
      "Neutral 0.8542992\n",
      "Neutral 0.84147096\n",
      "Neutral 0.91763383\n",
      "Positive 0.65151566\n",
      "Neutral 0.59575564\n",
      "Neutral 0.7234265\n",
      "Neutral 0.92039156\n",
      "Positive 0.8566892\n",
      "Neutral 0.92039156\n",
      "Neutral 0.92039156\n",
      "Neutral 0.720334\n",
      "Neutral 0.6710838\n",
      "Neutral 0.83264875\n",
      "Positive 0.9905586\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.720334\n",
      "Neutral 0.83264875\n",
      "Positive 0.9286576\n",
      "Neutral 0.83264875\n",
      "Neutral 0.8432555\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.55636954\n",
      "Negative 0.715599\n",
      "Neutral 0.88877887\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Positive 0.5710603\n",
      "Negative 0.70069754\n",
      "Neutral 0.598627\n",
      "Neutral 0.83264875\n",
      "Negative 0.9300637\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.7733134\n",
      "Neutral 0.92039156\n",
      "Neutral 0.78310883\n",
      "Neutral 0.83264875\n",
      "Positive 0.91638523\n",
      "Neutral 0.92039156\n",
      "Neutral 0.83264875\n",
      "Neutral 0.8393456\n",
      "Negative 0.9645741\n",
      "Negative 0.5932003\n",
      "Neutral 0.8202242\n",
      "Negative 0.8739497\n",
      "Negative 0.84697944\n",
      "Negative 0.9249491\n",
      "Negative 0.7373374\n",
      "Neutral 0.79431313\n",
      "Negative 0.7373374\n",
      "Neutral 0.7440286\n",
      "Negative 0.5600559\n",
      "Neutral 0.9450559\n",
      "Negative 0.67833275\n",
      "Neutral 0.49289128\n",
      "Negative 0.55741876\n",
      "Neutral 0.90956503\n",
      "Neutral 0.49918628\n",
      "Neutral 0.9190518\n",
      "Negative 0.51129746\n",
      "Negative 0.5087752\n",
      "Neutral 0.61689895\n",
      "Neutral 0.89459646\n",
      "Negative 0.7186845\n",
      "Neutral 0.7206011\n",
      "Neutral 0.7206011\n",
      "Neutral 0.9349407\n",
      "Neutral 0.826548\n",
      "Neutral 0.94032866\n",
      "Neutral 0.8336302\n",
      "Negative 0.61126536\n",
      "Neutral 0.66941005\n",
      "Positive 0.7226446\n",
      "Neutral 0.9322681\n",
      "Neutral 0.9322681\n",
      "Neutral 0.9322681\n",
      "Negative 0.7926934\n",
      "Neutral 0.70252204\n",
      "Positive 0.977464\n",
      "Positive 0.9736303\n",
      "Positive 0.9740371\n",
      "Neutral 0.54511327\n",
      "Neutral 0.7185709\n",
      "Positive 0.977464\n",
      "Negative 0.83803886\n",
      "Negative 0.93233466\n",
      "Negative 0.9525055\n",
      "Neutral 0.92039156\n",
      "Negative 0.83803886\n",
      "Neutral 0.63485605\n",
      "Neutral 0.7362332\n",
      "Neutral 0.42287362\n",
      "Positive 0.9364945\n",
      "Neutral 0.8634669\n",
      "Positive 0.9855824\n",
      "Neutral 0.8634669\n",
      "Negative 0.7391442\n",
      "Negative 0.91044134\n",
      "Positive 0.98785406\n",
      "Negative 0.5118393\n",
      "Negative 0.96482563\n",
      "Neutral 0.7185709\n",
      "Negative 0.96765393\n",
      "Negative 0.7496396\n",
      "Positive 0.97600085\n",
      "Positive 0.970385\n",
      "Negative 0.9028254\n",
      "Neutral 0.5055903\n",
      "Neutral 0.45647067\n",
      "Neutral 0.83264875\n",
      "Neutral 0.83264875\n",
      "Neutral 0.867961\n",
      "Neutral 0.82034713\n",
      "Neutral 0.881286\n",
      "Neutral 0.89843833\n",
      "Neutral 0.8407443\n",
      "Negative 0.9837653\n",
      "Neutral 0.8407443\n",
      "Neutral 0.75516105\n",
      "Positive 0.7484256\n",
      "Positive 0.9588752\n",
      "Positive 0.9660259\n",
      "Neutral 0.8184223\n",
      "Neutral 0.8493955\n",
      "Positive 0.93688065\n",
      "Neutral 0.83264875\n",
      "Neutral 0.92039156\n",
      "Positive 0.966635\n",
      "Positive 0.98231316\n",
      "Neutral 0.6244049\n",
      "Neutral 0.41287622\n",
      "Negative 0.9496686\n",
      "Negative 0.59297264\n",
      "Neutral 0.7504899\n",
      "Positive 0.9538673\n",
      "Positive 0.990417\n",
      "Negative 0.7897677\n",
      "Negative 0.72888476\n",
      "Positive 0.8753721\n",
      "Neutral 0.83186895\n",
      "Neutral 0.8714195\n",
      "Negative 0.5772398\n",
      "Negative 0.7231322\n",
      "Neutral 0.8402978\n",
      "Neutral 0.6701168\n",
      "Neutral 0.71893704\n",
      "Neutral 0.5642716\n",
      "Negative 0.75329995\n",
      "Positive 0.9143924\n",
      "Neutral 0.8430454\n",
      "Neutral 0.7300324\n",
      "Neutral 0.6701168\n",
      "Positive 0.9837716\n",
      "Neutral 0.8850492\n",
      "Positive 0.8792025\n",
      "Positive 0.9280683\n",
      "Positive 0.9773358\n",
      "Positive 0.84289396\n",
      "Positive 0.52039593\n",
      "Negative 0.94669\n",
      "Positive 0.89414805\n",
      "Neutral 0.78597164\n",
      "Neutral 0.8710794\n",
      "Neutral 0.7904863\n",
      "Neutral 0.45921758\n",
      "Neutral 0.7387349\n",
      "Neutral 0.63180196\n",
      "Positive 0.55284786\n",
      "Negative 0.8094877\n",
      "Negative 0.91032046\n",
      "Neutral 0.8442476\n",
      "Neutral 0.5813119\n",
      "Negative 0.9618678\n",
      "Neutral 0.7292199\n",
      "Negative 0.75784487\n",
      "Negative 0.83739114\n",
      "Neutral 0.8270398\n",
      "Positive 0.7714044\n",
      "Neutral 0.88877887\n",
      "Positive 0.8018358\n",
      "Neutral 0.78172535\n",
      "Negative 0.8362649\n",
      "Negative 0.91467834\n",
      "Negative 0.799859\n",
      "Negative 0.9029975\n",
      "Negative 0.6823679\n",
      "Negative 0.9360994\n",
      "Neutral 0.92039156\n",
      "Negative 0.7874035\n",
      "Positive 0.97600085\n",
      "Neutral 0.6969773\n",
      "Negative 0.8486974\n",
      "Negative 0.8486974\n",
      "Negative 0.8486974\n",
      "Negative 0.98271066\n",
      "Negative 0.7167049\n",
      "Negative 0.5461081\n",
      "Positive 0.92334867\n",
      "Neutral 0.9409028\n",
      "Positive 0.92834437\n",
      "Positive 0.91050965\n",
      "Neutral 0.77628684\n",
      "Neutral 0.5456151\n",
      "Negative 0.9608788\n",
      "Negative 0.48938152\n",
      "Neutral 0.67749834\n",
      "Neutral 0.5547116\n",
      "Neutral 0.52387947\n",
      "Neutral 0.5212769\n",
      "Negative 0.9367837\n",
      "Neutral 0.8173588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral 0.6868617\n",
      "Negative 0.56383795\n",
      "Neutral 0.6369138\n",
      "Neutral 0.8554057\n",
      "Positive 0.47056457\n",
      "Neutral 0.87230307\n",
      "Neutral 0.83264875\n",
      "Negative 0.575696\n",
      "Neutral 0.51499337\n",
      "Negative 0.9039189\n",
      "Neutral 0.78648424\n",
      "Neutral 0.78648424\n",
      "Negative 0.958592\n",
      "Negative 0.958592\n",
      "Neutral 0.5254828\n",
      "Neutral 0.5254828\n",
      "Negative 0.9002451\n",
      "Negative 0.85263383\n",
      "Neutral 0.7321868\n",
      "Neutral 0.8439727\n",
      "Neutral 0.6209354\n",
      "Neutral 0.8756371\n",
      "Neutral 0.71242994\n",
      "Positive 0.9757375\n",
      "Negative 0.9225361\n",
      "Neutral 0.7041294\n",
      "Neutral 0.6203606\n",
      "Negative 0.48938152\n",
      "Neutral 0.6235451\n",
      "Positive 0.96710616\n",
      "Neutral 0.50704\n",
      "Neutral 0.78162116\n",
      "Neutral 0.8810653\n",
      "Neutral 0.8810653\n",
      "Positive 0.51167476\n",
      "Neutral 0.78162116\n",
      "Positive 0.9068255\n",
      "Neutral 0.80604345\n",
      "Positive 0.51167476\n",
      "Neutral 0.701738\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Positive 0.75827014\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Negative 0.9126825\n",
      "Negative 0.6932433\n",
      "Positive 0.94376415\n",
      "Neutral 0.8574837\n",
      "Neutral 0.75916076\n",
      "Neutral 0.7168222\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8873203\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Positive 0.8519702\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.7473393\n",
      "Negative 0.8591782\n",
      "Negative 0.8591782\n",
      "Neutral 0.8574837\n",
      "Positive 0.51167476\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Positive 0.9847047\n",
      "Positive 0.9462951\n",
      "Neutral 0.75893843\n",
      "Neutral 0.4569373\n",
      "Positive 0.7566977\n",
      "Positive 0.73754853\n",
      "Negative 0.67639095\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.5740253\n",
      "Positive 0.6434357\n",
      "Positive 0.6768461\n",
      "Positive 0.86466247\n",
      "Positive 0.5321342\n",
      "Positive 0.6768461\n",
      "Positive 0.51167476\n",
      "Neutral 0.8574837\n",
      "Negative 0.90757734\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.82124054\n",
      "Neutral 0.86293095\n",
      "Neutral 0.8574837\n",
      "Neutral 0.62435156\n",
      "Neutral 0.78162116\n",
      "Positive 0.9385252\n",
      "Positive 0.680615\n",
      "Positive 0.8519702\n",
      "Positive 0.9610665\n",
      "Neutral 0.6315306\n",
      "Positive 0.57267356\n",
      "Negative 0.5396017\n",
      "Positive 0.9425535\n",
      "Neutral 0.8574837\n",
      "Neutral 0.9246485\n",
      "Neutral 0.91761374\n",
      "Negative 0.8591782\n",
      "Positive 0.9473981\n",
      "Neutral 0.55248135\n",
      "Positive 0.71833926\n",
      "Negative 0.7378964\n",
      "Neutral 0.6361735\n",
      "Neutral 0.8574837\n",
      "Negative 0.55891466\n",
      "Negative 0.9289497\n",
      "Neutral 0.65799034\n",
      "Neutral 0.9314409\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Neutral 0.8574837\n",
      "Negative 0.9289497\n",
      "Neutral 0.6203606\n",
      "Neutral 0.76618695\n",
      "Neutral 0.8574837\n",
      "Negative 0.8570582\n",
      "Positive 0.75301516\n",
      "Neutral 0.5548827\n",
      "Neutral 0.6888269\n",
      "Negative 0.51191294\n",
      "Neutral 0.67353594\n",
      "Positive 0.56685984\n",
      "Neutral 0.9376281\n",
      "Neutral 0.84343904\n",
      "Neutral 0.89167804\n",
      "Negative 0.59764355\n",
      "Neutral 0.78163147\n",
      "Negative 0.9184571\n",
      "Negative 0.49189878\n",
      "Positive 0.60056317\n",
      "Neutral 0.85778683\n",
      "Neutral 0.49333602\n",
      "Positive 0.87763363\n",
      "Negative 0.9018271\n",
      "Negative 0.42991462\n",
      "Negative 0.88248557\n",
      "Neutral 0.71102834\n",
      "Negative 0.9603294\n",
      "Positive 0.5069153\n",
      "Negative 0.78436947\n",
      "Negative 0.78436947\n",
      "Negative 0.8950998\n",
      "Positive 0.53513455\n",
      "Positive 0.972603\n",
      "Neutral 0.5235955\n",
      "Negative 0.97861594\n",
      "Neutral 0.4908266\n",
      "Negative 0.8926453\n",
      "Negative 0.9705489\n",
      "Negative 0.9704641\n",
      "Neutral 0.9034266\n",
      "Positive 0.7495416\n",
      "Negative 0.5810266\n",
      "Positive 0.8883673\n",
      "Positive 0.9855535\n",
      "Neutral 0.74276173\n",
      "Neutral 0.74276173\n",
      "Neutral 0.74276173\n",
      "Positive 0.88393825\n",
      "Neutral 0.63528854\n",
      "Positive 0.9823358\n",
      "Neutral 0.5213395\n",
      "Neutral 0.88417083\n",
      "Neutral 0.8544859\n",
      "Neutral 0.8806939\n",
      "Positive 0.978003\n",
      "Neutral 0.55611795\n",
      "Neutral 0.5211839\n",
      "Negative 0.7461449\n",
      "Positive 0.9743446\n",
      "Neutral 0.74276173\n",
      "Negative 0.5624431\n",
      "Neutral 0.61086583\n",
      "Neutral 0.68544686\n",
      "Negative 0.7644177\n",
      "Positive 0.5647375\n",
      "Positive 0.6219068\n",
      "Positive 0.6219068\n",
      "Positive 0.828218\n",
      "Neutral 0.8574837\n",
      "Neutral 0.74276173\n",
      "Positive 0.6208508\n",
      "Neutral 0.88174254\n",
      "Neutral 0.7689258\n",
      "Negative 0.73267025\n",
      "Positive 0.42673466\n",
      "Positive 0.50273615\n",
      "Neutral 0.7702402\n",
      "Positive 0.7204805\n",
      "Positive 0.7204805\n",
      "Neutral 0.84433603\n",
      "Neutral 0.6012712\n",
      "Neutral 0.8716285\n",
      "Neutral 0.91783565\n",
      "Neutral 0.84433603\n",
      "Positive 0.71979284\n",
      "Positive 0.9204339\n",
      "Neutral 0.8574837\n",
      "Positive 0.777587\n",
      "Negative 0.57248634\n",
      "Negative 0.48938152\n",
      "Neutral 0.90968823\n",
      "Positive 0.5715934\n",
      "Neutral 0.54763913\n",
      "Positive 0.88393825\n",
      "Positive 0.88393825\n",
      "Neutral 0.4512725\n",
      "Positive 0.8368325\n",
      "Neutral 0.71820337\n",
      "Positive 0.9040403\n",
      "Neutral 0.6718025\n",
      "Neutral 0.82179743\n",
      "Negative 0.75534075\n",
      "Positive 0.58272725\n",
      "Neutral 0.8810653\n",
      "Negative 0.8779193\n",
      "Neutral 0.63817745\n",
      "Neutral 0.91445726\n",
      "Neutral 0.6683102\n",
      "Negative 0.53103805\n",
      "Negative 0.96793014\n",
      "Negative 0.85550827\n",
      "Neutral 0.74033004\n",
      "Positive 0.98936695\n",
      "Negative 0.5425332\n",
      "Negative 0.4851242\n",
      "Neutral 0.8659695\n",
      "Neutral 0.79308146\n",
      "Negative 0.68384635\n",
      "Neutral 0.60277873\n",
      "Positive 0.8727847\n",
      "Neutral 0.637234\n",
      "Positive 0.90755916\n",
      "Neutral 0.733529\n",
      "Negative 0.73340696\n",
      "Neutral 0.79308146\n",
      "Positive 0.9500074\n",
      "Neutral 0.53317434\n",
      "Positive 0.6271639\n",
      "Positive 0.6271639\n",
      "Negative 0.5838267\n",
      "Positive 0.69226325\n",
      "Positive 0.9580226\n",
      "Negative 0.48226228\n",
      "Negative 0.7527255\n",
      "Positive 0.90755916\n",
      "Negative 0.6206155\n",
      "Negative 0.61211205\n",
      "Positive 0.8726625\n",
      "Neutral 0.7774561\n",
      "Negative 0.8762996\n",
      "Negative 0.7607887\n",
      "Negative 0.86697006\n",
      "Neutral 0.5213395\n",
      "Positive 0.90755916\n",
      "Positive 0.90755916\n",
      "Neutral 0.5642043\n",
      "Neutral 0.82469976\n",
      "Neutral 0.5694174\n",
      "Negative 0.69963896\n",
      "Positive 0.8726625\n",
      "Neutral 0.93398625\n",
      "Neutral 0.93398625\n",
      "Neutral 0.82880753\n",
      "Negative 0.8857385\n",
      "Negative 0.7872728\n",
      "Neutral 0.67756134\n",
      "Neutral 0.7557311\n",
      "Negative 0.6866694\n",
      "Positive 0.9253925\n",
      "Neutral 0.77011585\n",
      "Neutral 0.7185709\n",
      "Positive 0.9642171\n",
      "Negative 0.7685283\n",
      "Neutral 0.5185068\n",
      "Positive 0.5276442\n",
      "Neutral 0.51826084\n",
      "Negative 0.7235975\n",
      "Negative 0.49805772\n",
      "Neutral 0.6178981\n",
      "Positive 0.8726625\n",
      "Positive 0.6094149\n",
      "Negative 0.7046209\n",
      "Positive 0.9658001\n",
      "Neutral 0.6234535\n",
      "Positive 0.6094149\n",
      "Negative 0.5151183\n",
      "Neutral 0.5201678\n",
      "Positive 0.78017354\n",
      "Neutral 0.8816363\n",
      "Neutral 0.5201678\n",
      "Neutral 0.8432477\n",
      "Neutral 0.6562954\n",
      "Neutral 0.58078253\n",
      "Neutral 0.83971584\n",
      "Neutral 0.4934713\n",
      "Positive 0.90755916\n",
      "Neutral 0.6933519\n",
      "Neutral 0.8918036\n",
      "Neutral 0.4512725\n",
      "Neutral 0.9212222\n",
      "Neutral 0.5504842\n",
      "Negative 0.7755378\n",
      "Negative 0.6495398\n",
      "Positive 0.9556797\n",
      "Neutral 0.69031584\n",
      "Negative 0.91446567\n",
      "Negative 0.5332163\n",
      "Neutral 0.69817686\n",
      "Negative 0.9212787\n",
      "Neutral 0.78279537\n",
      "Neutral 0.45066673\n",
      "Negative 0.9212787\n",
      "Neutral 0.9280449\n",
      "Positive 0.51451164\n",
      "Neutral 0.85569197\n",
      "Positive 0.9827353\n",
      "Neutral 0.7076518\n",
      "Neutral 0.4512725\n",
      "Neutral 0.7634778\n",
      "Neutral 0.81716454\n",
      "Positive 0.6992597\n",
      "Neutral 0.8329752\n",
      "Positive 0.9580226\n",
      "Neutral 0.819325\n",
      "Neutral 0.51678085\n",
      "Negative 0.8407927\n",
      "Neutral 0.819325\n",
      "Neutral 0.819325\n",
      "Positive 0.9383138\n",
      "Positive 0.9580226\n",
      "Neutral 0.8214464\n",
      "Neutral 0.89043814\n",
      "Neutral 0.892546\n",
      "Positive 0.9743325\n",
      "Neutral 0.6386813\n",
      "Neutral 0.819325\n",
      "Neutral 0.6729452\n",
      "Neutral 0.90774244\n",
      "Positive 0.9744961\n",
      "Positive 0.94735926\n",
      "Negative 0.8954706\n",
      "Positive 0.88393825\n",
      "Negative 0.6017368\n",
      "Positive 0.83269006\n",
      "Positive 0.90755916\n",
      "Negative 0.953633\n",
      "Negative 0.953633\n",
      "Negative 0.953633\n",
      "Negative 0.953633\n",
      "Neutral 0.8432555\n",
      "Neutral 0.8300581\n",
      "Neutral 0.81030077\n",
      "Positive 0.92750347\n",
      "Negative 0.5588398\n",
      "Positive 0.92750347\n",
      "Negative 0.9792335\n",
      "Neutral 0.7371226\n",
      "Negative 0.522553\n",
      "Negative 0.49163663\n",
      "Neutral 0.737457\n",
      "Neutral 0.91506803\n",
      "Neutral 0.521896\n",
      "Neutral 0.91190964\n",
      "Negative 0.7731029\n",
      "Neutral 0.91190964\n",
      "Neutral 0.91190964\n",
      "Neutral 0.72933686\n",
      "Neutral 0.91190964\n",
      "Neutral 0.91190964\n",
      "Negative 0.7731029\n",
      "Positive 0.79584974\n",
      "Positive 0.85296494\n",
      "Negative 0.54384136\n",
      "Positive 0.90755916\n",
      "Neutral 0.91190964\n",
      "Positive 0.9108693\n",
      "Neutral 0.91190964\n",
      "Neutral 0.91190964\n",
      "Neutral 0.5409622\n",
      "Negative 0.61487776\n",
      "Negative 0.61487776\n",
      "Neutral 0.819325\n",
      "Neutral 0.91445726\n",
      "Neutral 0.8568137\n",
      "Positive 0.8415831\n",
      "Positive 0.94631714\n",
      "Negative 0.6527914\n",
      "Neutral 0.5635048\n",
      "Positive 0.9580226\n",
      "Negative 0.963795\n",
      "Neutral 0.91190964\n",
      "Negative 0.9684623\n",
      "Neutral 0.69779885\n",
      "Neutral 0.79640484\n",
      "Negative 0.77848506\n",
      "Neutral 0.8706815\n",
      "Positive 0.8870916\n",
      "Neutral 0.89155734\n",
      "Neutral 0.5611354\n",
      "Positive 0.6094149\n",
      "Negative 0.7673434\n",
      "Negative 0.8052149\n",
      "Neutral 0.7884891\n",
      "Neutral 0.91190964\n",
      "Positive 0.9356271\n",
      "Positive 0.96026003\n",
      "Neutral 0.62213254\n",
      "Neutral 0.85983527\n",
      "Neutral 0.91190964\n",
      "Positive 0.6094149\n",
      "Neutral 0.59736985\n",
      "Neutral 0.4512725\n",
      "Neutral 0.6711495\n",
      "Negative 0.74586964\n",
      "Negative 0.83875394\n",
      "Neutral 0.7004087\n",
      "Positive 0.6094149\n",
      "Positive 0.9552487\n",
      "Neutral 0.7004087\n",
      "Negative 0.6186086\n",
      "Neutral 0.8506686\n",
      "Positive 0.6094149\n",
      "Neutral 0.94479394\n",
      "Neutral 0.7838512\n",
      "Neutral 0.80634385\n",
      "Neutral 0.94479394\n",
      "Positive 0.96877927\n",
      "Negative 0.9096521\n",
      "Neutral 0.65398437\n",
      "Neutral 0.4697049\n",
      "Neutral 0.6640115\n",
      "Negative 0.5043883\n",
      "Neutral 0.5910818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral 0.71117115\n",
      "Positive 0.9083532\n",
      "Negative 0.65682113\n",
      "Negative 0.7719654\n",
      "Positive 0.96877927\n",
      "Neutral 0.67705405\n",
      "Neutral 0.67705405\n",
      "Neutral 0.8782284\n",
      "Neutral 0.8498749\n",
      "Positive 0.9580226\n",
      "Neutral 0.66379523\n",
      "Neutral 0.9032981\n",
      "Neutral 0.719998\n",
      "Positive 0.53513455\n",
      "Positive 0.9827353\n",
      "Neutral 0.90793866\n",
      "Neutral 0.90793866\n",
      "Neutral 0.91243845\n",
      "Neutral 0.91098106\n",
      "Negative 0.7855163\n",
      "Neutral 0.9058469\n",
      "Negative 0.7855163\n",
      "Negative 0.66959447\n",
      "Negative 0.71510047\n",
      "Neutral 0.9289441\n",
      "Neutral 0.6041992\n",
      "Positive 0.58444583\n",
      "Neutral 0.6041992\n",
      "Neutral 0.819325\n",
      "Positive 0.94129795\n",
      "Negative 0.824431\n",
      "Neutral 0.91190964\n",
      "Neutral 0.6286973\n",
      "Neutral 0.6041992\n",
      "Positive 0.97543675\n",
      "Neutral 0.64901525\n",
      "Positive 0.9089728\n",
      "Neutral 0.8816363\n",
      "Neutral 0.62765694\n",
      "Positive 0.90755916\n",
      "Neutral 0.652732\n",
      "Negative 0.7369431\n",
      "Neutral 0.51802105\n",
      "Neutral 0.8374863\n",
      "Neutral 0.51802105\n",
      "Neutral 0.51802105\n",
      "Neutral 0.79630333\n",
      "Neutral 0.51802105\n",
      "Neutral 0.80201167\n",
      "Neutral 0.9201364\n",
      "Neutral 0.6041992\n",
      "Positive 0.96659625\n",
      "Positive 0.6922202\n",
      "Neutral 0.9201364\n",
      "Neutral 0.9201364\n",
      "Neutral 0.9201364\n",
      "Positive 0.9553585\n",
      "Positive 0.9609057\n",
      "Neutral 0.72439116\n",
      "Positive 0.96613157\n",
      "Neutral 0.91190964\n",
      "Neutral 0.8485508\n",
      "Neutral 0.56647843\n",
      "Neutral 0.9201364\n",
      "Neutral 0.753872\n",
      "Neutral 0.4512725\n",
      "Neutral 0.819325\n",
      "Positive 0.57137555\n",
      "Neutral 0.51802105\n",
      "Positive 0.81986386\n",
      "Neutral 0.8473187\n",
      "Neutral 0.819325\n",
      "Neutral 0.63003623\n",
      "Negative 0.69233197\n",
      "Neutral 0.819325\n",
      "Negative 0.51349455\n",
      "Neutral 0.70290905\n",
      "Neutral 0.54190516\n",
      "Negative 0.779141\n",
      "Negative 0.8961963\n",
      "Neutral 0.653893\n",
      "Positive 0.9731636\n",
      "Positive 0.9905757\n",
      "Neutral 0.8634393\n",
      "Neutral 0.90769464\n",
      "Positive 0.97436017\n",
      "Neutral 0.9048015\n",
      "Neutral 0.819325\n",
      "Neutral 0.9091958\n",
      "Negative 0.81210464\n",
      "Neutral 0.90749985\n",
      "Neutral 0.60502785\n",
      "Neutral 0.6271128\n",
      "Positive 0.9735606\n",
      "Positive 0.9769913\n",
      "Neutral 0.68013436\n",
      "Neutral 0.51802105\n",
      "Neutral 0.6232798\n",
      "Neutral 0.51802105\n",
      "Neutral 0.9469737\n",
      "Neutral 0.7774953\n",
      "Negative 0.87990874\n",
      "Negative 0.9188879\n",
      "Positive 0.9356271\n",
      "Positive 0.84586614\n",
      "Neutral 0.6375758\n",
      "Neutral 0.7026326\n",
      "Neutral 0.6171722\n",
      "Neutral 0.51802105\n",
      "Positive 0.97948414\n",
      "Neutral 0.78235054\n",
      "Neutral 0.878816\n",
      "Positive 0.97332186\n",
      "Positive 0.87933016\n",
      "Positive 0.8519702\n",
      "Neutral 0.8022997\n",
      "Neutral 0.9019505\n",
      "Neutral 0.8396782\n",
      "Neutral 0.8574837\n",
      "Neutral 0.9074166\n",
      "Positive 0.5450574\n",
      "Neutral 0.56027913\n",
      "Positive 0.99035084\n",
      "Neutral 0.4654348\n",
      "Positive 0.9304057\n",
      "Positive 0.9042856\n",
      "Negative 0.49808004\n",
      "Positive 0.9048431\n",
      "Positive 0.97948414\n",
      "Positive 0.9236327\n",
      "Neutral 0.4512725\n",
      "Neutral 0.89489895\n",
      "Positive 0.97170436\n",
      "Neutral 0.49685258\n",
      "Neutral 0.6521206\n",
      "Negative 0.91809475\n",
      "Neutral 0.819325\n",
      "Negative 0.9058837\n",
      "Neutral 0.8773143\n",
      "Positive 0.8457822\n",
      "Neutral 0.8678141\n",
      "Negative 0.547271\n",
      "Neutral 0.815071\n",
      "Positive 0.9855201\n",
      "Neutral 0.8051595\n",
      "Neutral 0.4512725\n",
      "Negative 0.6151989\n",
      "Neutral 0.6151216\n",
      "Positive 0.94585305\n",
      "Neutral 0.680041\n",
      "Neutral 0.88121605\n",
      "Positive 0.977403\n",
      "Negative 0.5147132\n",
      "Neutral 0.91375434\n",
      "Neutral 0.8747217\n",
      "Neutral 0.9316033\n",
      "Neutral 0.51344776\n",
      "Neutral 0.7704116\n",
      "Positive 0.76439613\n",
      "Neutral 0.51344776\n",
      "Neutral 0.8446384\n",
      "Neutral 0.69175804\n",
      "Neutral 0.84269804\n",
      "Positive 0.96576995\n",
      "Negative 0.90750116\n",
      "Positive 0.98660773\n",
      "Positive 0.7723546\n",
      "Neutral 0.7884163\n",
      "Negative 0.94665974\n",
      "Neutral 0.85127234\n",
      "Neutral 0.88847184\n",
      "Negative 0.4894941\n",
      "Positive 0.8906791\n",
      "Neutral 0.9316033\n",
      "Negative 0.94665974\n",
      "Neutral 0.75843036\n",
      "Positive 0.6894931\n",
      "Positive 0.71678436\n",
      "Positive 0.71678436\n",
      "Positive 0.8159622\n",
      "Neutral 0.9316033\n",
      "Neutral 0.9514205\n",
      "Neutral 0.9514205\n",
      "Positive 0.62145346\n",
      "Positive 0.7165253\n",
      "Positive 0.71678436\n",
      "Neutral 0.89416134\n",
      "Neutral 0.89416134\n",
      "Positive 0.5699355\n",
      "Neutral 0.89416134\n",
      "Positive 0.9217324\n",
      "Neutral 0.92204964\n",
      "Neutral 0.92204964\n",
      "Neutral 0.4809792\n",
      "Neutral 0.7037376\n",
      "Negative 0.77091944\n",
      "Neutral 0.8849916\n",
      "Neutral 0.7734287\n",
      "Positive 0.86502063\n",
      "Neutral 0.6941442\n",
      "Neutral 0.6408368\n",
      "Positive 0.64793843\n",
      "Neutral 0.6041992\n",
      "Negative 0.94955254\n",
      "Positive 0.79161984\n",
      "Neutral 0.66674685\n",
      "Positive 0.49669746\n",
      "Neutral 0.92997617\n",
      "Positive 0.9480612\n",
      "Neutral 0.4512725\n",
      "Neutral 0.934515\n",
      "Neutral 0.9306939\n",
      "Negative 0.8528817\n",
      "Positive 0.8416978\n",
      "Neutral 0.523584\n",
      "Positive 0.97516394\n",
      "Positive 0.64221936\n",
      "Neutral 0.89416134\n",
      "Negative 0.9821115\n",
      "Positive 0.9296886\n",
      "Neutral 0.8816048\n",
      "Neutral 0.95324415\n",
      "Neutral 0.9179341\n",
      "Neutral 0.4512725\n",
      "Neutral 0.4512725\n",
      "Negative 0.79278153\n",
      "Neutral 0.4512725\n",
      "Neutral 0.4512725\n",
      "Neutral 0.943831\n",
      "Neutral 0.87541115\n",
      "Neutral 0.7537537\n",
      "Neutral 0.94300187\n",
      "Positive 0.6360402\n",
      "Positive 0.8565283\n",
      "Neutral 0.92997617\n",
      "Positive 0.9480612\n",
      "Neutral 0.7728397\n",
      "Negative 0.75207245\n",
      "Negative 0.9051198\n",
      "Neutral 0.65673953\n",
      "Negative 0.5615541\n",
      "Neutral 0.5387813\n",
      "Neutral 0.5387813\n",
      "Neutral 0.8943227\n",
      "Positive 0.97617525\n",
      "Positive 0.8274897\n",
      "Neutral 0.8132619\n",
      "Neutral 0.819325\n",
      "Positive 0.98550886\n",
      "Positive 0.953286\n",
      "Neutral 0.7948695\n",
      "Neutral 0.92997617\n",
      "Negative 0.69528353\n",
      "Negative 0.5807317\n",
      "Positive 0.9753187\n",
      "Neutral 0.6468123\n",
      "Neutral 0.92997617\n",
      "Positive 0.9517968\n",
      "Positive 0.6094149\n",
      "Positive 0.8665275\n",
      "Positive 0.7425379\n",
      "Negative 0.85417634\n",
      "Positive 0.9325644\n",
      "Negative 0.775191\n",
      "Negative 0.465588\n",
      "Negative 0.45923272\n",
      "Neutral 0.6430145\n",
      "Negative 0.8864476\n",
      "Neutral 0.81623244\n",
      "Neutral 0.66262937\n",
      "Negative 0.8584382\n",
      "Negative 0.822423\n",
      "Positive 0.936014\n",
      "Negative 0.7444875\n",
      "Neutral 0.651034\n",
      "Negative 0.8334605\n",
      "Neutral 0.81467956\n",
      "Neutral 0.9048921\n",
      "Neutral 0.9048921\n",
      "Neutral 0.9048921\n",
      "Positive 0.82044965\n",
      "Neutral 0.9048921\n",
      "Neutral 0.9048921\n",
      "Negative 0.94408464\n",
      "Negative 0.88380826\n",
      "Negative 0.8905283\n",
      "Neutral 0.6663441\n",
      "Positive 0.7913141\n",
      "Neutral 0.819325\n",
      "Neutral 0.8574837\n",
      "Neutral 0.6203606\n",
      "Negative 0.48938152\n",
      "Neutral 0.6928965\n",
      "Negative 0.8649367\n",
      "Negative 0.6967032\n",
      "Negative 0.87749875\n",
      "Neutral 0.701346\n",
      "Negative 0.5658065\n",
      "Neutral 0.7827475\n",
      "Neutral 0.8562324\n",
      "Neutral 0.8507289\n",
      "Neutral 0.80318666\n",
      "Neutral 0.47002062\n",
      "Positive 0.9706392\n",
      "Positive 0.7692441\n",
      "Negative 0.93230283\n",
      "Negative 0.7145656\n",
      "Positive 0.936014\n",
      "Positive 0.62058705\n",
      "Positive 0.5092688\n",
      "Neutral 0.8487689\n",
      "Neutral 0.85426897\n",
      "Negative 0.48938152\n",
      "Neutral 0.6018221\n",
      "Neutral 0.9048921\n",
      "Neutral 0.9048921\n",
      "Positive 0.84701586\n",
      "Positive 0.86222845\n",
      "Positive 0.59463906\n",
      "Positive 0.56251377\n",
      "Neutral 0.7260144\n",
      "Positive 0.68069345\n",
      "Negative 0.6520344\n",
      "Negative 0.65761393\n",
      "Neutral 0.9048921\n",
      "Positive 0.5893952\n",
      "Neutral 0.8353963\n",
      "Neutral 0.7399493\n",
      "Positive 0.8361191\n",
      "Positive 0.90146655\n",
      "Positive 0.86222845\n",
      "Negative 0.65670216\n",
      "Negative 0.6675121\n",
      "Positive 0.9872839\n",
      "Positive 0.90765697\n",
      "Neutral 0.4512725\n",
      "Positive 0.52598906\n",
      "Neutral 0.4512725\n",
      "Negative 0.7185963\n",
      "Neutral 0.4512725\n",
      "Neutral 0.4512725\n",
      "Neutral 0.5843518\n",
      "Negative 0.5914536\n",
      "Neutral 0.6522635\n",
      "Neutral 0.41305447\n",
      "Negative 0.79037637\n",
      "Neutral 0.7797689\n",
      "Neutral 0.8687637\n",
      "Neutral 0.8942088\n",
      "Neutral 0.8453263\n",
      "Neutral 0.5172704\n",
      "Neutral 0.73469067\n",
      "Neutral 0.77224845\n",
      "Neutral 0.9066423\n",
      "Neutral 0.5068241\n",
      "Neutral 0.6769586\n",
      "Negative 0.693035\n",
      "Negative 0.6350785\n",
      "Negative 0.58737713\n",
      "Negative 0.6292686\n",
      "Negative 0.607163\n",
      "Neutral 0.53544927\n",
      "Positive 0.86222845\n",
      "Negative 0.650206\n",
      "Neutral 0.8376873\n",
      "Positive 0.56961566\n",
      "Positive 0.6216933\n",
      "Neutral 0.670535\n",
      "Neutral 0.80501634\n",
      "Negative 0.75880426\n",
      "Neutral 0.80848926\n",
      "Neutral 0.57637966\n",
      "Neutral 0.57637966\n",
      "Negative 0.78023046\n",
      "Negative 0.78023046\n",
      "Negative 0.8657863\n",
      "Neutral 0.7328792\n",
      "Negative 0.68081355\n",
      "Neutral 0.9184641\n",
      "Negative 0.66172135\n",
      "Neutral 0.6381055\n",
      "Negative 0.78023046\n",
      "Negative 0.9562165\n",
      "Neutral 0.6240545\n",
      "Positive 0.6496572\n",
      "Neutral 0.57637966\n",
      "Negative 0.61933815\n",
      "Negative 0.9708339\n",
      "Neutral 0.8353088\n",
      "Neutral 0.5555073\n",
      "Neutral 0.80501634\n",
      "Neutral 0.7372348\n",
      "Positive 0.409752\n",
      "Neutral 0.78317124\n",
      "Neutral 0.6350937\n",
      "Neutral 0.78679645\n",
      "Neutral 0.78679645\n",
      "Negative 0.78247356\n",
      "Neutral 0.78662205\n",
      "Positive 0.62004757\n",
      "Negative 0.97663885\n",
      "Positive 0.9800996\n",
      "Positive 0.971035\n",
      "Neutral 0.7079049\n",
      "Negative 0.9017432\n",
      "Positive 0.9733023\n",
      "Positive 0.97507846\n",
      "Negative 0.9017432\n",
      "Negative 0.63568157\n",
      "Positive 0.97507846\n",
      "Neutral 0.9208123\n",
      "Neutral 0.85465103\n",
      "Positive 0.86502063\n",
      "Neutral 0.57637966\n",
      "Neutral 0.9261491\n",
      "Positive 0.96205384\n",
      "Negative 0.71920395\n",
      "Neutral 0.7513024\n",
      "Neutral 0.8198155\n",
      "Neutral 0.92997617\n",
      "Negative 0.7333438\n",
      "Negative 0.9396019\n",
      "Neutral 0.5833728\n",
      "Negative 0.55629456\n",
      "Negative 0.9690981\n",
      "Neutral 0.5212769\n",
      "Neutral 0.705765\n",
      "Neutral 0.9464957\n",
      "Neutral 0.8353088\n",
      "Neutral 0.8996556\n",
      "Neutral 0.50164545\n",
      "Negative 0.78480065\n",
      "Positive 0.9826411\n",
      "Neutral 0.5521819\n",
      "Neutral 0.862324\n",
      "Negative 0.7714897\n",
      "Negative 0.9457545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.78480065\n",
      "Neutral 0.6310912\n",
      "Neutral 0.5610386\n",
      "Neutral 0.52994806\n",
      "Positive 0.8379569\n",
      "Neutral 0.4512725\n",
      "Positive 0.70762515\n",
      "Neutral 0.91431004\n",
      "Negative 0.9669078\n",
      "Positive 0.83911324\n",
      "Negative 0.68201375\n",
      "Positive 0.93871254\n",
      "Neutral 0.94307435\n",
      "Neutral 0.83215266\n",
      "Negative 0.55516255\n",
      "Neutral 0.68923503\n",
      "Negative 0.7275902\n",
      "Neutral 0.9261491\n",
      "Positive 0.96620923\n",
      "Neutral 0.61575425\n",
      "Positive 0.86222845\n",
      "Positive 0.79392767\n",
      "Positive 0.9845309\n",
      "Positive 0.9700373\n",
      "Neutral 0.8773696\n",
      "Negative 0.923269\n",
      "Negative 0.9145619\n",
      "Positive 0.9520642\n",
      "Neutral 0.69941336\n",
      "Neutral 0.7804786\n",
      "Neutral 0.88741595\n",
      "Neutral 0.49892136\n",
      "Negative 0.7070926\n",
      "Neutral 0.79743135\n",
      "Neutral 0.7845329\n",
      "Negative 0.6146255\n",
      "Neutral 0.61152345\n",
      "Positive 0.9509105\n",
      "Negative 0.6864096\n",
      "Negative 0.760011\n",
      "Neutral 0.5638102\n",
      "Neutral 0.9003076\n",
      "Neutral 0.6903203\n",
      "Negative 0.9023187\n",
      "Negative 0.9297443\n",
      "Neutral 0.55689967\n",
      "Neutral 0.66674745\n",
      "Positive 0.8598741\n",
      "Neutral 0.8695744\n",
      "Positive 0.9773091\n",
      "Negative 0.92101187\n",
      "Neutral 0.6360473\n",
      "Neutral 0.8695744\n",
      "Negative 0.4319511\n",
      "Neutral 0.9184641\n",
      "Neutral 0.7397099\n",
      "Neutral 0.78786373\n",
      "Neutral 0.89821094\n",
      "Negative 0.83566856\n",
      "Neutral 0.69150305\n",
      "Neutral 0.8478266\n",
      "Neutral 0.59911615\n",
      "Negative 0.7939115\n",
      "Positive 0.9849746\n",
      "Neutral 0.57637966\n",
      "Negative 0.8765834\n",
      "Negative 0.52641225\n",
      "Negative 0.80954605\n",
      "Positive 0.95080566\n",
      "Neutral 0.7703865\n",
      "Positive 0.9432387\n",
      "Positive 0.94525874\n",
      "Neutral 0.7703865\n",
      "Neutral 0.57632315\n",
      "Negative 0.85882413\n",
      "Positive 0.9356271\n",
      "Positive 0.84701586\n",
      "Negative 0.6057128\n",
      "Positive 0.94099814\n",
      "Positive 0.86222845\n",
      "Positive 0.8518948\n",
      "Neutral 0.80138797\n",
      "Positive 0.9617726\n",
      "Neutral 0.8330345\n",
      "Positive 0.7275559\n",
      "Negative 0.64595103\n",
      "Positive 0.89048827\n",
      "Positive 0.8119285\n",
      "Negative 0.47859025\n",
      "Positive 0.69493103\n",
      "Neutral 0.6787882\n",
      "Neutral 0.8825575\n",
      "Neutral 0.83471817\n",
      "Neutral 0.80844754\n",
      "Neutral 0.63869977\n",
      "Negative 0.61726695\n",
      "Negative 0.75080806\n",
      "Negative 0.61726695\n",
      "Neutral 0.7354242\n",
      "Positive 0.50801057\n",
      "Neutral 0.48510674\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9403439\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.86222845\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.73858774\n",
      "Neutral 0.5034563\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.96190065\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Negative 0.5872545\n",
      "Positive 0.9856877\n",
      "Positive 0.96563476\n",
      "Neutral 0.9000221\n",
      "Positive 0.9856877\n",
      "Positive 0.98981637\n",
      "Positive 0.9856877\n",
      "Negative 0.7532107\n",
      "Positive 0.95008534\n",
      "Positive 0.9856877\n",
      "Positive 0.77488333\n",
      "Positive 0.9856877\n",
      "Negative 0.77887064\n",
      "Negative 0.77887064\n",
      "Positive 0.9790689\n",
      "Negative 0.6748573\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.51895195\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.5289678\n",
      "Negative 0.81872237\n",
      "Positive 0.9856877\n",
      "Negative 0.7106331\n",
      "Negative 0.7244535\n",
      "Neutral 0.9184641\n",
      "Positive 0.9856877\n",
      "Negative 0.9237563\n",
      "Neutral 0.76526225\n",
      "Neutral 0.86215216\n",
      "Neutral 0.5159812\n",
      "Negative 0.93045765\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.98103595\n",
      "Positive 0.9856877\n",
      "Positive 0.94788355\n",
      "Neutral 0.91806805\n",
      "Negative 0.7964071\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.7808467\n",
      "Positive 0.9856877\n",
      "Neutral 0.7808467\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.8884019\n",
      "Neutral 0.9184641\n",
      "Positive 0.9856877\n",
      "Neutral 0.86803687\n",
      "Positive 0.9856877\n",
      "Positive 0.9412692\n",
      "Positive 0.94906634\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.97116697\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Negative 0.77260554\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Negative 0.48938152\n",
      "Neutral 0.65108424\n",
      "Neutral 0.62981665\n",
      "Negative 0.9695917\n",
      "Negative 0.9146572\n",
      "Positive 0.9856877\n",
      "Negative 0.90854675\n",
      "Negative 0.8876155\n",
      "Positive 0.6434885\n",
      "Positive 0.9856877\n",
      "Negative 0.92730755\n",
      "Positive 0.9922369\n",
      "Negative 0.92062396\n",
      "Neutral 0.587723\n",
      "Positive 0.53090733\n",
      "Neutral 0.670337\n",
      "Positive 0.579509\n",
      "Negative 0.81019175\n",
      "Negative 0.863288\n",
      "Neutral 0.62981665\n",
      "Neutral 0.6817058\n",
      "Neutral 0.62981665\n",
      "Neutral 0.71788263\n",
      "Negative 0.9622132\n",
      "Negative 0.56856626\n",
      "Neutral 0.7302416\n",
      "Neutral 0.7302416\n",
      "Positive 0.9657333\n",
      "Negative 0.9470435\n",
      "Negative 0.92349064\n",
      "Positive 0.9856877\n",
      "Positive 0.9898504\n",
      "Neutral 0.53598136\n",
      "Neutral 0.62981665\n",
      "Neutral 0.5290706\n",
      "Positive 0.96576995\n",
      "Neutral 0.78176093\n",
      "Neutral 0.6813284\n",
      "Positive 0.9458939\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.94172215\n",
      "Positive 0.579509\n",
      "Negative 0.7732808\n",
      "Positive 0.9856877\n",
      "Neutral 0.8089799\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Negative 0.8499569\n",
      "Negative 0.80954605\n",
      "Negative 0.9017432\n",
      "Negative 0.45923272\n",
      "Neutral 0.51344776\n",
      "Neutral 0.64228666\n",
      "Negative 0.9212787\n",
      "Neutral 0.4908266\n",
      "Neutral 0.78648424\n",
      "Neutral 0.5573163\n",
      "Positive 0.5500538\n",
      "Positive 0.8136437\n",
      "Neutral 0.4255923\n",
      "Negative 0.7483488\n",
      "Negative 0.61904025\n",
      "Negative 0.9551865\n",
      "Positive 0.52443326\n",
      "Negative 0.9399251\n",
      "Neutral 0.7399312\n",
      "Positive 0.9856877\n",
      "Neutral 0.696641\n",
      "Negative 0.542383\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Positive 0.4936571\n",
      "Neutral 0.9013363\n",
      "Positive 0.9856877\n",
      "Neutral 0.5277209\n",
      "Neutral 0.8662206\n",
      "Neutral 0.64966077\n",
      "Neutral 0.5212769\n",
      "Positive 0.4936571\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.8574837\n",
      "Neutral 0.85778683\n",
      "Positive 0.9856877\n",
      "Positive 0.96576995\n",
      "Neutral 0.93969977\n",
      "Positive 0.8148455\n",
      "Positive 0.745572\n",
      "Negative 0.97663885\n",
      "Positive 0.96203506\n",
      "Negative 0.6670178\n",
      "Neutral 0.5573163\n",
      "Neutral 0.5573163\n",
      "Negative 0.9678272\n",
      "Positive 0.65612715\n",
      "Positive 0.67086107\n",
      "Positive 0.9924474\n",
      "Neutral 0.8485598\n",
      "Positive 0.9740551\n",
      "Neutral 0.7801931\n",
      "Neutral 0.85457945\n",
      "Neutral 0.8046486\n",
      "Neutral 0.74614614\n",
      "Negative 0.91765237\n",
      "Neutral 0.7025196\n",
      "Positive 0.9856877\n",
      "Neutral 0.65479875\n",
      "Neutral 0.50255483\n",
      "Neutral 0.81055796\n",
      "Positive 0.9856877\n",
      "Negative 0.7998549\n",
      "Neutral 0.90786797\n",
      "Negative 0.9133094\n",
      "Neutral 0.8250048\n",
      "Negative 0.6528438\n",
      "Positive 0.9856877\n",
      "Neutral 0.80371624\n",
      "Negative 0.45912218\n",
      "Neutral 0.7589917\n",
      "Neutral 0.86425114\n",
      "Neutral 0.57962286\n",
      "Negative 0.6465148\n",
      "Positive 0.8856392\n",
      "Negative 0.9308156\n",
      "Negative 0.84183246\n",
      "Negative 0.596829\n",
      "Neutral 0.6506142\n",
      "Neutral 0.9334475\n",
      "Positive 0.97714597\n",
      "Neutral 0.770199\n",
      "Negative 0.9309216\n",
      "Neutral 0.8308793\n",
      "Neutral 0.5241812\n",
      "Negative 0.82548374\n",
      "Neutral 0.84482604\n",
      "Neutral 0.93448246\n",
      "Positive 0.96563476\n",
      "Negative 0.80847603\n",
      "Positive 0.7753943\n",
      "Positive 0.87757856\n",
      "Neutral 0.8327278\n",
      "Neutral 0.62981665\n",
      "Neutral 0.89275914\n",
      "Positive 0.8083703\n",
      "Positive 0.96802354\n",
      "Positive 0.92872363\n",
      "Positive 0.8632974\n",
      "Negative 0.88681334\n",
      "Positive 0.9856877\n",
      "Neutral 0.5878763\n",
      "Neutral 0.53388166\n",
      "Neutral 0.8456344\n",
      "Neutral 0.78993225\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.6942723\n",
      "Neutral 0.8556873\n",
      "Negative 0.6677542\n",
      "Positive 0.9403056\n",
      "Positive 0.70290816\n",
      "Positive 0.9856877\n",
      "Negative 0.7863789\n",
      "Negative 0.69927245\n",
      "Negative 0.88222426\n",
      "Negative 0.78916925\n",
      "Neutral 0.6447141\n",
      "Positive 0.9856877\n",
      "Negative 0.6874508\n",
      "Positive 0.9856877\n",
      "Neutral 0.75271505\n",
      "Negative 0.8720664\n",
      "Negative 0.8720664\n",
      "Positive 0.9657333\n",
      "Positive 0.9856877\n",
      "Negative 0.62637025\n",
      "Positive 0.63647324\n",
      "Positive 0.9856877\n",
      "Neutral 0.6419279\n",
      "Neutral 0.5398536\n",
      "Negative 0.8717673\n",
      "Positive 0.9761271\n",
      "Neutral 0.8845421\n",
      "Neutral 0.5225989\n",
      "Neutral 0.84056944\n",
      "Neutral 0.7292497\n",
      "Neutral 0.8905233\n",
      "Neutral 0.6127637\n",
      "Positive 0.76007295\n",
      "Negative 0.5347609\n",
      "Negative 0.7202831\n",
      "Negative 0.761151\n",
      "Negative 0.9395008\n",
      "Negative 0.5347609\n",
      "Positive 0.98799855\n",
      "Positive 0.78188086\n",
      "Negative 0.5225134\n",
      "Positive 0.7483107\n",
      "Positive 0.8121242\n",
      "Positive 0.8121242\n",
      "Negative 0.85639256\n",
      "Negative 0.7340922\n",
      "Negative 0.63984936\n",
      "Positive 0.9482559\n",
      "Positive 0.9767947\n",
      "Positive 0.9856877\n",
      "Neutral 0.86523867\n",
      "Neutral 0.8701192\n",
      "Negative 0.64855826\n",
      "Neutral 0.5537725\n",
      "Neutral 0.87631315\n",
      "Negative 0.9775931\n",
      "Neutral 0.51460135\n",
      "Neutral 0.881427\n",
      "Neutral 0.49283212\n",
      "Negative 0.885\n",
      "Neutral 0.8062994\n",
      "Neutral 0.48315415\n",
      "Neutral 0.89738345\n",
      "Positive 0.9892233\n",
      "Neutral 0.62655497\n",
      "Neutral 0.68771553\n",
      "Neutral 0.75036275\n",
      "Neutral 0.81270677\n",
      "Positive 0.9569009\n",
      "Neutral 0.8655461\n",
      "Neutral 0.84945315\n",
      "Neutral 0.71472\n",
      "Negative 0.5347609\n",
      "Neutral 0.64077985\n",
      "Negative 0.53049\n",
      "Neutral 0.6700398\n",
      "Neutral 0.48646808\n",
      "Neutral 0.7974977\n",
      "Negative 0.8955942\n",
      "Neutral 0.7974977\n",
      "Neutral 0.5683298\n",
      "Neutral 0.5836412\n",
      "Neutral 0.66003394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.6917686\n",
      "Positive 0.7646868\n",
      "Negative 0.8024364\n",
      "Neutral 0.54674387\n",
      "Negative 0.68196017\n",
      "Negative 0.529585\n",
      "Positive 0.973963\n",
      "Positive 0.9753875\n",
      "Positive 0.9753875\n",
      "Positive 0.68848854\n",
      "Neutral 0.60784537\n",
      "Negative 0.777016\n",
      "Neutral 0.74572307\n",
      "Neutral 0.54674387\n",
      "Neutral 0.56926906\n",
      "Neutral 0.61200505\n",
      "Positive 0.64481\n",
      "Neutral 0.77459306\n",
      "Neutral 0.5045196\n",
      "Negative 0.9169253\n",
      "Negative 0.9598613\n",
      "Neutral 0.59274507\n",
      "Positive 0.9891676\n",
      "Positive 0.8698731\n",
      "Positive 0.89541906\n",
      "Positive 0.89541906\n",
      "Negative 0.49186233\n",
      "Neutral 0.4709541\n",
      "Negative 0.6685042\n",
      "Negative 0.75747633\n",
      "Positive 0.6196035\n",
      "Neutral 0.6638531\n",
      "Positive 0.6799231\n",
      "Neutral 0.79098165\n",
      "Positive 0.97838825\n",
      "Negative 0.5692539\n",
      "Negative 0.5692539\n",
      "Positive 0.6196035\n",
      "Positive 0.95354533\n",
      "Negative 0.60339063\n",
      "Neutral 0.81602436\n",
      "Neutral 0.534097\n",
      "Negative 0.84934115\n",
      "Neutral 0.6184065\n",
      "Negative 0.8342778\n",
      "Neutral 0.734537\n",
      "Neutral 0.7536761\n",
      "Neutral 0.627901\n",
      "Neutral 0.6191274\n",
      "Negative 0.9536787\n",
      "Neutral 0.66625553\n",
      "Neutral 0.76685953\n",
      "Neutral 0.71128714\n",
      "Negative 0.4903227\n",
      "Negative 0.69497275\n",
      "Positive 0.7228388\n",
      "Positive 0.9856877\n",
      "Positive 0.9856877\n",
      "Neutral 0.8601279\n",
      "Neutral 0.8276117\n",
      "Neutral 0.8864491\n",
      "Neutral 0.84556705\n",
      "Neutral 0.8864491\n",
      "Neutral 0.8864491\n",
      "Neutral 0.8864491\n",
      "Positive 0.9856877\n",
      "Positive 0.8566343\n",
      "Neutral 0.83810145\n",
      "Neutral 0.83810145\n",
      "Neutral 0.83810145\n",
      "Neutral 0.83810145\n",
      "Neutral 0.6700398\n",
      "Neutral 0.83810145\n",
      "Negative 0.74003035\n",
      "Neutral 0.69685125\n",
      "Positive 0.94625556\n",
      "Neutral 0.8630654\n",
      "Positive 0.5399161\n",
      "Neutral 0.5475539\n",
      "Neutral 0.5475539\n",
      "Neutral 0.5475539\n",
      "Neutral 0.9310371\n",
      "Neutral 0.56096184\n",
      "Neutral 0.9310371\n",
      "Negative 0.70744365\n",
      "Neutral 0.9255434\n",
      "Neutral 0.8692879\n",
      "Neutral 0.5475539\n",
      "Neutral 0.9310371\n",
      "Neutral 0.8692879\n",
      "Neutral 0.9310371\n",
      "Neutral 0.647612\n",
      "Negative 0.7819772\n",
      "Negative 0.49753794\n",
      "Positive 0.53785366\n",
      "Positive 0.9856877\n",
      "Negative 0.51416975\n",
      "Negative 0.51416975\n",
      "Positive 0.98463\n",
      "Negative 0.6670178\n",
      "Positive 0.8746724\n",
      "Negative 0.97663885\n",
      "Negative 0.79302526\n",
      "Positive 0.8554557\n",
      "Negative 0.8250276\n",
      "Negative 0.84425044\n",
      "Neutral 0.85259354\n",
      "Positive 0.5803298\n",
      "Positive 0.3909989\n",
      "Positive 0.62909937\n",
      "Neutral 0.5679324\n",
      "Negative 0.6970912\n",
      "Neutral 0.8924355\n",
      "Positive 0.5803298\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "\n",
    "roberta_model = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "train_model = AutoModelForSequenceClassification.from_pretrained(roberta_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta_model)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "tweet_score_lst = []\n",
    "tweet_label_lst = []\n",
    "for index, row in df.iterrows():\n",
    "#     print(row['text'])\n",
    "    # sentiment analysis\n",
    "#     tokenized_tweet = tokenizer.encode(row['text'], padding=True, truncation=True, max_length=100, add_special_tokens = True)\n",
    "    tokenized_tweet = tokenizer(row['text'], return_tensors='pt')\n",
    "    # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
    "    output = train_model(**tokenized_tweet)\n",
    "\n",
    "    roberta_scores = output[0][0].detach().numpy()\n",
    "    roberta_scores = softmax(roberta_scores)\n",
    "    \n",
    "    tweet_scores = max(roberta_scores)\n",
    "    tweet_label = labels[np.argmax(roberta_scores)]\n",
    "    tweet_score_lst.append(tweet_scores)\n",
    "    tweet_label_lst.append(tweet_label)\n",
    "    print(tweet_label,tweet_scores )\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa4e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_score_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a063ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(tweet_score_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85ad08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20211090\\AppData\\Local\\Temp\\ipykernel_2836\\1131194265.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['score'] = np.array(tweet_score_lst)\n",
      "C:\\Users\\20211090\\AppData\\Local\\Temp\\ipykernel_2836\\1131194265.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = np.array(tweet_label_lst)\n"
     ]
    }
   ],
   "source": [
    "df['score'] = np.array(tweet_score_lst)\n",
    "df['label'] = np.array(tweet_label_lst)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f2aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>. fly Amsterdam-Bangalore now that Jet Airways...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>launch thrice weekly service Amsterdam using a...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "      <td>0.921999</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Thanks Dan</td>\n",
       "      <td>2409245946</td>\n",
       "      <td>en</td>\n",
       "      <td>0.647281</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Aviation releases about 3% human carbon emissi...</td>\n",
       "      <td>1109522354206584833</td>\n",
       "      <td>en</td>\n",
       "      <td>0.820563</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>DO NOT FLY EVER AGAIN OR How is one person alo...</td>\n",
       "      <td>56459096</td>\n",
       "      <td>en</td>\n",
       "      <td>0.850238</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>Thanks, I already went through this list „š. S...</td>\n",
       "      <td>538129774</td>\n",
       "      <td>en</td>\n",
       "      <td>0.629099</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>Hi Maddie. I sincerely apologize any inconveni...</td>\n",
       "      <td>5920532</td>\n",
       "      <td>en</td>\n",
       "      <td>0.567932</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>Thank you, but I've contacted you guys already...</td>\n",
       "      <td>1918871</td>\n",
       "      <td>en</td>\n",
       "      <td>0.697091</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>Home Royal Dutch Airlines</td>\n",
       "      <td>31592791</td>\n",
       "      <td>en</td>\n",
       "      <td>0.892435</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>V-shaped Flying Wing concept by uses two engin...</td>\n",
       "      <td>759460420352630788</td>\n",
       "      <td>en</td>\n",
       "      <td>0.580330</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              user_id  \\\n",
       "921   . fly Amsterdam-Bangalore now that Jet Airways...           1328529979   \n",
       "922   launch thrice weekly service Amsterdam using a...           1328529979   \n",
       "923                                          Thanks Dan           2409245946   \n",
       "925   Aviation releases about 3% human carbon emissi...  1109522354206584833   \n",
       "926   DO NOT FLY EVER AGAIN OR How is one person alo...             56459096   \n",
       "...                                                 ...                  ...   \n",
       "5010  Thanks, I already went through this list „š. S...            538129774   \n",
       "5011  Hi Maddie. I sincerely apologize any inconveni...              5920532   \n",
       "5013  Thank you, but I've contacted you guys already...              1918871   \n",
       "5018                          Home Royal Dutch Airlines             31592791   \n",
       "5020  V-shaped Flying Wing concept by uses two engin...   759460420352630788   \n",
       "\n",
       "     lang     score     label  \n",
       "921    en  0.713487   Neutral  \n",
       "922    en  0.921999   Neutral  \n",
       "923    en  0.647281  Positive  \n",
       "925    en  0.820563  Negative  \n",
       "926    en  0.850238  Negative  \n",
       "...   ...       ...       ...  \n",
       "5010   en  0.629099  Positive  \n",
       "5011   en  0.567932   Neutral  \n",
       "5013   en  0.697091  Negative  \n",
       "5018   en  0.892435   Neutral  \n",
       "5020   en  0.580330  Positive  \n",
       "\n",
       "[1862 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e65f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAMKL24050 HAVE NOT RECEIVED MY BAGS FROM THE FLIGHT'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5000] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bf0130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'][5000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a37433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f9d2b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     895\n",
       "Positive    527\n",
       "Negative    440\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69dbcf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>. fly Amsterdam-Bangalore now that Jet Airways...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>launch thrice weekly service Amsterdam using a...</td>\n",
       "      <td>1328529979</td>\n",
       "      <td>en</td>\n",
       "      <td>0.921999</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Thanks Dan</td>\n",
       "      <td>2409245946</td>\n",
       "      <td>en</td>\n",
       "      <td>0.647281</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Aviation releases about 3% human carbon emissi...</td>\n",
       "      <td>1109522354206584833</td>\n",
       "      <td>en</td>\n",
       "      <td>0.820563</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>DO NOT FLY EVER AGAIN OR How is one person alo...</td>\n",
       "      <td>56459096</td>\n",
       "      <td>en</td>\n",
       "      <td>0.850238</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>Thanks, I already went through this list „š. S...</td>\n",
       "      <td>538129774</td>\n",
       "      <td>en</td>\n",
       "      <td>0.629099</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>Hi Maddie. I sincerely apologize any inconveni...</td>\n",
       "      <td>5920532</td>\n",
       "      <td>en</td>\n",
       "      <td>0.567932</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>Thank you, but I've contacted you guys already...</td>\n",
       "      <td>1918871</td>\n",
       "      <td>en</td>\n",
       "      <td>0.697091</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>Home Royal Dutch Airlines</td>\n",
       "      <td>31592791</td>\n",
       "      <td>en</td>\n",
       "      <td>0.892435</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>V-shaped Flying Wing concept by uses two engin...</td>\n",
       "      <td>759460420352630788</td>\n",
       "      <td>en</td>\n",
       "      <td>0.580330</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text              user_id  \\\n",
       "921   . fly Amsterdam-Bangalore now that Jet Airways...           1328529979   \n",
       "922   launch thrice weekly service Amsterdam using a...           1328529979   \n",
       "923                                          Thanks Dan           2409245946   \n",
       "925   Aviation releases about 3% human carbon emissi...  1109522354206584833   \n",
       "926   DO NOT FLY EVER AGAIN OR How is one person alo...             56459096   \n",
       "...                                                 ...                  ...   \n",
       "5010  Thanks, I already went through this list „š. S...            538129774   \n",
       "5011  Hi Maddie. I sincerely apologize any inconveni...              5920532   \n",
       "5013  Thank you, but I've contacted you guys already...              1918871   \n",
       "5018                          Home Royal Dutch Airlines             31592791   \n",
       "5020  V-shaped Flying Wing concept by uses two engin...   759460420352630788   \n",
       "\n",
       "     lang     score     label  \n",
       "921    en  0.713487   Neutral  \n",
       "922    en  0.921999   Neutral  \n",
       "923    en  0.647281  Positive  \n",
       "925    en  0.820563  Negative  \n",
       "926    en  0.850238  Negative  \n",
       "...   ...       ...       ...  \n",
       "5010   en  0.629099  Positive  \n",
       "5011   en  0.567932   Neutral  \n",
       "5013   en  0.697091  Negative  \n",
       "5018   en  0.892435   Neutral  \n",
       "5020   en  0.580330  Positive  \n",
       "\n",
       "[1862 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d114b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "625cfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_copy = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont try to run this cell, the kernel will die I dont know why\n",
    "# Create dataset\n",
    "# height = [895, 527,440]\n",
    "# bars = ('Neutral', 'Positive', 'Negative')\n",
    "# x_pos = np.arange(len(bars))\n",
    " \n",
    "# # Create bars\n",
    "# plt.bar(x_pos, height)\n",
    " \n",
    "# # Create names on the x-axis\n",
    "# plt.xticks(x_pos, bars)\n",
    " \n",
    "# # Show graphic\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa646ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dont try to run this cell, the kernel will die I dont know why\n",
    "# Sentiment Distribution Plot\n",
    "# data_plot = df['label'].value_counts()\n",
    "# plt.title('Sentiment Analysis', fontsize= 15, fontweight='bold')\n",
    "# plt.xlabel('Label')\n",
    "# plt.ylabel('number of tweets')\n",
    "# df_plot.plot(kind ='bar')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6e8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#using function is restricted as having RuntimeError: target size 712 exceed size of tensor (512) at non-singleton dimension 1\n",
    "# solution:see how to do chunking https://huggingface.co/course/chapter7/6?fw=tf\n",
    "\n",
    "# # define a function to get roberta_scores\n",
    "\n",
    "# def get_robeta_scores(text):\n",
    "    \n",
    "#     # sentiment analysis\n",
    "#     tokenized_tweet = tokenizer(text, return_tensors='pt')\n",
    "#     # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
    "#     output = train_model(**tokenized_tweet)\n",
    "\n",
    "#     roberta_scores = output[0][0].detach().numpy()\n",
    "#     roberta_scores = softmax(roberta_scores)\n",
    "\n",
    "#     return max(roberta_scores)\n",
    "\n",
    "# # df_klm['score'] = df_klm['text'].apply(get_robeta_scores)\n",
    "# # print(get_robeta_scores(df_klm['text']))\n",
    "\n",
    "# text1 = 'Today is wonderful'\n",
    "# get_robeta_scores(text1)\n",
    "\n",
    "# df_klm['score'] = df_klm['text'].apply(get_robeta_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
